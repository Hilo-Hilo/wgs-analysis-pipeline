name: WGS Pipeline Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Run tests weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'

env:
  PYTHON_VERSION: '3.11'
  CONDA_ENV: 'wgs_analysis'

jobs:
  shellcheck-lint:
    name: ShellCheck Lint
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install ShellCheck
        run: |
          sudo apt-get update
          sudo apt-get install -y shellcheck

      - name: Run ShellCheck on shell scripts
        shell: bash
        run: |
          mapfile -t shell_scripts < <(find . -type f -name "*.sh" -not -path "./.git/*" | sort)

          if [[ ${#shell_scripts[@]} -eq 0 ]]; then
            echo "No shell scripts found."
            exit 0
          fi

          printf 'Checking %s\n' "${shell_scripts[@]}"
          shellcheck -S error -e SC2034,SC2155,SC2086 "${shell_scripts[@]}"

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: [shellcheck-lint]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Run unit tests (no conda/toolchain)
        run: bash tests/run_tests.sh --unit-only

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [unit-tests]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Setup Miniconda
        uses: conda-incubator/setup-miniconda@v3
        with:
          auto-update-conda: true
          activate-environment: ${{ env.CONDA_ENV }}
          environment-file: .github/conda/integration-environment.yml
          channels: conda-forge,bioconda
          conda-remove-defaults: true
          channel-priority: flexible
          use-mamba: true

      - name: Verify bioinformatics toolchain
        shell: bash -el {0}
        run: |
          fastqc --version || true
          fastp --version || true
          bwa 2>&1 | head -3 || true
          samtools --version || true
          bcftools --version || true

      - name: Run full test suite
        shell: bash -el {0}
        run: bash tests/run_tests.sh

      - name: Upload integration test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-outputs-integration
          path: |
            /tmp/wgs_pipeline_test_*/
            tests/sample_data/
          retention-days: 7

  docker:
    name: Docker Build & Test
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'pull_request'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        run: |
          echo "ðŸ³ Building Docker image..."
          docker build \
            --build-arg BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ') \
            --build-arg BUILD_VERSION=${{ github.sha }} \
            --build-arg VCS_REF=${{ github.sha }} \
            -t wgs-pipeline:test .

      - name: Test Docker image
        run: |
          echo "ðŸ§ª Testing Docker image..."

          # Test that the image runs
          docker run --rm wgs-pipeline:test check_requirements.sh --help

          # ENTRYPOINT already executes inside conda env (wgs_analysis)
          docker run --rm wgs-pipeline:test fastqc --version
          docker run --rm wgs-pipeline:test fastp --version
          docker run --rm wgs-pipeline:test bash -lc "bwa 2>&1 | head -1"
          docker run --rm wgs-pipeline:test bash -lc "samtools --version | head -1"
          docker run --rm wgs-pipeline:test bash -lc "bcftools --version | head -1"

          echo "âœ… Docker image tests passed!"

  docs:
    name: Documentation Check
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check documentation files
        run: |
          echo "ðŸ“š Checking documentation..."

          # Check that required documentation exists
          required_docs=(
            "README.md"
            "GETTING_STARTED.md"
            "16GB_SYSTEM_GUIDE.md"
            "TROUBLESHOOTING.md"
            "INPUT_OUTPUT_SPECIFICATION.md"
          )

          for doc in "${required_docs[@]}"; do
            if [[ -f "$doc" ]]; then
              echo "âœ… Found: $doc"
            else
              echo "âŒ Missing: $doc"
              exit 1
            fi
          done

          # Check for broken links (basic)
          grep -r "\[.*\](.*\.md)" *.md | while read -r line; do
            file=$(echo "$line" | cut -d: -f1)
            link=$(echo "$line" | grep -o '\[.*\](.*\.md)' | sed 's/.*](\(.*\))/\1/')

            if [[ -f "$link" ]]; then
              echo "âœ… Valid link in $file: $link"
            else
              echo "âŒ Broken link in $file: $link"
              # Don't exit here, just report
            fi
          done

          echo "ðŸ“– Documentation check completed!"

  benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    needs: [integration-tests]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Miniconda
        uses: conda-incubator/setup-miniconda@v3
        with:
          auto-update-conda: true
          python-version: ${{ env.PYTHON_VERSION }}
          channels: conda-forge,bioconda
          conda-remove-defaults: true
          channel-priority: flexible
          use-mamba: true

      - name: Install tools
        shell: bash -el {0}
        run: |
          conda create -n ${{ env.CONDA_ENV }} -c conda-forge -c bioconda \
            python=${{ env.PYTHON_VERSION }} fastqc fastp -y
          conda activate ${{ env.CONDA_ENV }}

      - name: Run performance benchmark
        shell: bash -el {0}
        run: |
          conda activate ${{ env.CONDA_ENV }}

          echo "âš¡ Running performance benchmark..."
          mkdir -p logs
          python3 tests/generate_sample_data.py --num-reads 10000 --output-dir benchmark_data

          # Time the quality control step
          time_start=$(date +%s)
          scripts/quality_control.sh \
            --input-dir benchmark_data \
            --output-dir benchmark_results \
            --threads 2
          time_end=$(date +%s)

          duration=$((time_end - time_start))
          echo "ðŸ“Š QC completed in ${duration} seconds for 10,000 reads"

          # Store benchmark result
          echo "benchmark_duration_seconds: $duration" > benchmark_results.txt
          echo "benchmark_reads: 10000" >> benchmark_results.txt
          echo "benchmark_date: $(date -u)" >> benchmark_results.txt

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmark_results.txt
            benchmark_results/
          retention-days: 30
